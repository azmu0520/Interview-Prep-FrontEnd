# Space Complexity (Big O Notation) - Complete Deep Dive

## üéØ Key Concepts

### What Space Complexity Actually Means

**Space complexity** describes how much memory an algorithm needs as input size increases. Just like time complexity, it's about the **rate of growth**, not exact bytes.

**Think of it like this:**

- O(1) space: "No matter how big the input, same memory"
- O(n) space: "Double the input, double the memory"
- O(n¬≤) space: "Double the input, quadruple the memory"
- O(log n) space: "Double the input, add one more level"

---

## üìä Space vs Time: The Fundamental Tradeoff

**Critical Understanding:**

```
You can often trade space for time (and vice versa)

More memory ‚Üí Faster execution
Less memory ‚Üí Slower execution
```

**Real-World Example:**

### Python

```python
# Approach 1: Time-optimized (uses extra space)
def has_duplicate_fast(arr):
    seen = set()  # O(n) space
    for num in arr:  # O(n) time
        if num in seen:
            return True
        seen.add(num)
    return False

# Approach 2: Space-optimized (slower)
def has_duplicate_slow(arr):
    # O(1) space - no extra data structure
    for i in range(len(arr)):  # O(n¬≤) time
        for j in range(i + 1, len(arr)):
            if arr[i] == arr[j]:
                return True
    return False

# Fast version: O(n) time, O(n) space
# Slow version: O(n¬≤) time, O(1) space
```

### JavaScript

```javascript
// Approach 1: Time-optimized (uses extra space)
function hasDuplicateFast(arr) {
  const seen = new Set(); // O(n) space
  for (const num of arr) {
    // O(n) time
    if (seen.has(num)) {
      return true;
    }
    seen.add(num);
  }
  return false;
}

// Approach 2: Space-optimized (slower)
function hasDuplicateSlow(arr) {
  // O(1) space - no extra data structure
  for (let i = 0; i < arr.length; i++) {
    // O(n¬≤) time
    for (let j = i + 1; j < arr.length; j++) {
      if (arr[i] === arr[j]) {
        return true;
      }
    }
  }
  return false;
}

// Fast version: O(n) time, O(n) space
// Slow version: O(n¬≤) time, O(1) space
```

---

## üîë Two Types of Space

### 1. **Input Space** (Usually Ignored)

Space taken by the input itself - we **don't count this** in analysis.

**Python:**

```python
def process(arr):  # arr takes O(n) space
    return arr[0]

# Space complexity: O(1)
# We DON'T count input space
# Input is given, we can't control it
```

**JavaScript:**

```javascript
function process(arr) {
  // arr takes O(n) space
  return arr[0];
}

// Space complexity: O(1)
// We DON'T count input space
// Input is given, we can't control it
```

### 2. **Auxiliary Space** (What We Measure!)

Extra space used **beyond the input** - this is what space complexity refers to.

**Python:**

```python
def create_copy(arr):
    result = []  # O(n) auxiliary space
    for item in arr:
        result.append(item)
    return result

# Space complexity: O(n) auxiliary space
# We created a new array same size as input
```

**JavaScript:**

```javascript
function createCopy(arr) {
  const result = []; // O(n) auxiliary space
  for (const item of arr) {
    result.push(item);
  }
  return result;
}

// Space complexity: O(n) auxiliary space
// We created a new array same size as input
```

**Interview Gold:**

> "When I analyze space complexity, I focus on **auxiliary space**‚Äîthe extra memory my algorithm allocates beyond the input. Input space is fixed and given to us, so we don't count it."

---

## 1Ô∏è‚É£ O(1) - Constant Space

### What it means:

**Memory usage doesn't change regardless of input size**

### Real-World Examples:

**Example 1: Simple Variable Storage**

**Python:**

```python
def find_max(arr):
    max_val = arr[0]  # O(1) - single variable

    for num in arr:
        if num > max_val:
            max_val = num  # Still just one variable

    return max_val

# No matter if arr has 10 or 10 million elements
# We only use one extra variable: max_val
# Space: O(1)
```

**JavaScript:**

```javascript
function findMax(arr) {
  let maxVal = arr[0]; // O(1) - single variable

  for (const num of arr) {
    if (num > maxVal) {
      maxVal = num; // Still just one variable
    }
  }

  return maxVal;
}

// No matter if arr has 10 or 10 million elements
// We only use one extra variable: maxVal
// Space: O(n) for call stack
// Each call stores: n, return address, local variables
```

### Iterative vs Recursive Space

**Python:**

```python
# RECURSIVE: O(n) space (call stack)
def sum_recursive(arr, index=0):
    if index >= len(arr):
        return 0
    return arr[index] + sum_recursive(arr, index + 1)

# ITERATIVE: O(1) space
def sum_iterative(arr):
    total = 0
    for num in arr:
        total += num
    return total

# Same functionality, different space complexity!
```

**JavaScript:**

```javascript
// RECURSIVE: O(n) space (call stack)
function sumRecursive(arr, index = 0) {
  if (index >= arr.length) {
    return 0;
  }
  return arr[index] + sumRecursive(arr, index + 1);
}

// ITERATIVE: O(1) space
function sumIterative(arr) {
  let total = 0;
  for (const num of arr) {
    total += num;
  }
  return total;
}

// Same functionality, different space complexity!
```

### Tail Recursion Optimization

**Python:**

```python
# NOT tail recursive: O(n) space
def factorial(n):
    if n <= 1:
        return 1
    return n * factorial(n - 1)  # Operation after recursive call

# TAIL recursive: Can be optimized to O(1) space
# (Note: Python doesn't optimize tail recursion, but the pattern matters)
def factorial_tail(n, accumulator=1):
    if n <= 1:
        return accumulator
    return factorial_tail(n - 1, n * accumulator)  # Last operation is call

# Tail recursion: recursive call is the LAST thing
# Some languages optimize this to O(1) space
```

**JavaScript:**

```javascript
// NOT tail recursive: O(n) space
function factorial(n) {
  if (n <= 1) {
    return 1;
  }
  return n * factorial(n - 1); // Operation after recursive call
}

// TAIL recursive: Can be optimized to O(1) space
// (Note: Most JS engines don't optimize tail recursion yet)
function factorialTail(n, accumulator = 1) {
  if (n <= 1) {
    return accumulator;
  }
  return factorialTail(n - 1, n * accumulator); // Last operation is call
}

// Tail recursion: recursive call is the LAST thing
// Some languages optimize this to O(1) space
```

### Key Understanding Points:

- ‚úÖ Each recursive call = one stack frame
- ‚úÖ Maximum recursion depth = space complexity
- ‚úÖ Iterative often uses less space than recursive
- ‚úÖ Tail recursion can be optimized (language-dependent)
- üéØ Deep recursion can cause stack overflow

### Interview Gold:

> "When analyzing recursive algorithms, I always consider the call stack. The maximum depth of recursion determines space complexity. For example, binary search is O(log n) space recursively but O(1) iteratively. If space is a concern, I can often convert recursion to iteration."

---

## üéØ In-Place Algorithms

### What "In-Place" Means

Modifies the input structure directly with O(1) extra space

**Python:**

```python
# ‚ùå NOT in-place: O(n) space
def reverse_not_in_place(arr):
    return arr[::-1]  # Creates new array

# ‚úÖ IN-PLACE: O(1) space
def reverse_in_place(arr):
    left, right = 0, len(arr) - 1
    while left < right:
        arr[left], arr[right] = arr[right], arr[left]
        left += 1
        right -= 1
    return arr  # Modified original array

# Both reverse the array
# But space complexity differs!
```

**JavaScript:**

```javascript
// ‚ùå NOT in-place: O(n) space
function reverseNotInPlace(arr) {
  return arr.slice().reverse(); // Creates new array
}

// ‚úÖ IN-PLACE: O(1) space
function reverseInPlace(arr) {
  let left = 0,
    right = arr.length - 1;
  while (left < right) {
    [arr[left], arr[right]] = [arr[right], arr[left]];
    left++;
    right--;
  }
  return arr; // Modified original array
}

// Both reverse the array
// But space complexity differs!
```

### Common In-Place Algorithms

**Example 1: Remove Duplicates from Sorted Array**

**Python:**

```python
def remove_duplicates(nums):
    if not nums:
        return 0

    write_ptr = 1  # O(1) space - just pointers

    for read_ptr in range(1, len(nums)):
        if nums[read_ptr] != nums[read_ptr - 1]:
            nums[write_ptr] = nums[read_ptr]
            write_ptr += 1

    return write_ptr

# Modifies original array
# Space: O(1)
```

**JavaScript:**

```javascript
function removeDuplicates(nums) {
  if (nums.length === 0) {
    return 0;
  }

  let writePtr = 1; // O(1) space - just pointers

  for (let readPtr = 1; readPtr < nums.length; readPtr++) {
    if (nums[readPtr] !== nums[readPtr - 1]) {
      nums[writePtr] = nums[readPtr];
      writePtr++;
    }
  }

  return writePtr;
}

// Modifies original array
// Space: O(1)
```

**Example 2: Partition Array (Quick Sort)**

**Python:**

```python
def partition(arr, low, high):
    pivot = arr[high]
    i = low - 1

    for j in range(low, high):
        if arr[j] <= pivot:
            i += 1
            arr[i], arr[j] = arr[j], arr[i]  # Swap in-place

    arr[i + 1], arr[high] = arr[high], arr[i + 1]
    return i + 1

# Rearranges elements in original array
# Space: O(1)
```

**JavaScript:**

```javascript
function partition(arr, low, high) {
  const pivot = arr[high];
  let i = low - 1;

  for (let j = low; j < high; j++) {
    if (arr[j] <= pivot) {
      i++;
      [arr[i], arr[j]] = [arr[j], arr[i]]; // Swap in-place
    }
  }

  [arr[i + 1], arr[high]] = [arr[high], arr[i + 1]];
  return i + 1;
}

// Rearranges elements in original array
// Space: O(1)
```

**Example 3: Rotate Array**

**Python:**

```python
def rotate(arr, k):
    n = len(arr)
    k = k % n

    # Reverse entire array
    reverse(arr, 0, n - 1)
    # Reverse first k elements
    reverse(arr, 0, k - 1)
    # Reverse remaining elements
    reverse(arr, k, n - 1)

def reverse(arr, start, end):
    while start < end:
        arr[start], arr[end] = arr[end], arr[start]
        start += 1
        end -= 1

# Three reversals, all in-place
# Space: O(1)
```

**JavaScript:**

```javascript
function rotate(arr, k) {
  const n = arr.length;
  k = k % n;

  // Reverse entire array
  reverse(arr, 0, n - 1);
  // Reverse first k elements
  reverse(arr, 0, k - 1);
  // Reverse remaining elements
  reverse(arr, k, n - 1);
}

function reverse(arr, start, end) {
  while (start < end) {
    [arr[start], arr[end]] = [arr[end], arr[start]];
    start++;
    end--;
  }
}

// Three reversals, all in-place
// Space: O(1)
```

### Key Understanding Points:

- ‚úÖ Modifies original data structure
- ‚úÖ Uses O(1) auxiliary space
- ‚úÖ Often uses two-pointer technique
- ‚úÖ Trade-off: mutates input (side effect)
- üéØ In-place = O(1) auxiliary space

### Interview Gold:

> "When I say 'in-place algorithm,' I mean we modify the input directly using only constant extra space. This is valuable for memory-constrained environments. The trade-off is that we lose the original data‚Äîsomething to discuss with the interviewer if data preservation matters."

---

## üìä Space Complexity of Common Data Structures

### Array/List Operations

| Operation          | Time   | Space |
| ------------------ | ------ | ----- |
| Access by index    | O(1)   | O(1)  |
| Append to end      | O(1)\* | O(1)  |
| Insert at position | O(n)   | O(1)  |
| Delete at position | O(n)   | O(1)  |
| Slice/Copy         | O(k)   | O(k)  |

\*Amortized

### Hash Table (Dictionary/Map)

| Operation     | Time | Space |
| ------------- | ---- | ----- |
| Store n items | -    | O(n)  |
| Insert/Delete | O(1) | O(1)  |
| Lookup        | O(1) | O(1)  |
| Iteration     | O(n) | O(1)  |

### Tree Operations

| Operation       | Time | Space    |
| --------------- | ---- | -------- |
| Store n nodes   | -    | O(n)     |
| DFS (recursive) | O(n) | O(h)\*   |
| DFS (iterative) | O(n) | O(h)     |
| BFS             | O(n) | O(w)\*\* |

\*h = height, \*\*w = max width

### Sorting Algorithms

| Algorithm      | Time       | Space      |
| -------------- | ---------- | ---------- |
| Merge Sort     | O(n log n) | O(n)       |
| Quick Sort     | O(n log n) | O(log n)\* |
| Heap Sort      | O(n log n) | O(1)       |
| Bubble Sort    | O(n¬≤)      | O(1)       |
| Insertion Sort | O(n¬≤)      | O(1)       |

\*Call stack

---

## üî• Space Complexity Analysis Rules

### Rule 1: Don't Count Input Space

**Python:**

```python
def process(arr):  # arr is input - don't count
    total = 0  # This counts: O(1)
    return total

# Space: O(1) auxiliary
```

**JavaScript:**

```javascript
function process(arr) {
  // arr is input - don't count
  let total = 0; // This counts: O(1)
  return total;
}

// Space: O(1) auxiliary
```

### Rule 2: Count All Auxiliary Structures

**Python:**

```python
def example(arr):
    hash_set = set()  # O(n)
    result = []       # O(n)
    temp = 0          # O(1)

    # Total: O(n + n + 1) = O(2n + 1) = O(n)
```

**JavaScript:**

```javascript
function example(arr) {
  const hashSet = new Set(); // O(n)
  const result = []; // O(n)
  let temp = 0; // O(1)

  // Total: O(n + n + 1) = O(2n + 1) = O(n)
}
```

### Rule 3: Count Maximum Stack Depth for Recursion

**Python:**

```python
def dfs(node):
    if not node:
        return
    dfs(node.left)   # Adds to stack
    dfs(node.right)  # Adds to stack

# Maximum stack depth = tree height
# Balanced tree: O(log n)
# Skewed tree: O(n)
```

**JavaScript:**

```javascript
function dfs(node) {
  if (!node) {
    return;
  }
  dfs(node.left); // Adds to stack
  dfs(node.right); // Adds to stack
}

// Maximum stack depth = tree height
// Balanced tree: O(log n)
// Skewed tree: O(n)
```

### Rule 4: Temporary Variables Are Usually O(1)

**Python:**

```python
def swap_and_process(arr):
    # All these are O(1):
    temp = arr[0]
    i = 0
    j = len(arr) - 1
    flag = True
    count = 0

    # Total: O(1) even though 5 variables
```

**JavaScript:**

```javascript
function swapAndProcess(arr) {
  // All these are O(1):
  const temp = arr[0];
  let i = 0;
  let j = arr.length - 1;
  let flag = true;
  let count = 0;

  // Total: O(1) even though 5 variables
}
```

### Rule 5: Output Space Sometimes Counts

Depends on the problem context!

**Python:**

```python
# If the problem REQUIRES returning result array:
def get_squares(arr):
    return [x*x for x in arr]  # O(n) but REQUIRED output

# Some interviews count output space
# Some don't (since it's required by problem)
# ALWAYS ASK THE INTERVIEWER!
```

**JavaScript:**

```javascript
// If the problem REQUIRES returning result array:
function getSquares(arr) {
  return arr.map((x) => x * x); // O(n) but REQUIRED output
}

// Some interviews count output space
// Some don't (since it's required by problem)
// ALWAYS ASK THE INTERVIEWER!
```

---

## üé§ Top Interview Questions & Model Answers

### Q1: What is space complexity and how is it different from time complexity?

**Perfect Answer:**

"Space complexity measures how much memory an algorithm uses relative to input size, while time complexity measures execution time. Both use Big O notation.

The key distinction is what we measure: space complexity tracks auxiliary space‚Äîextra memory beyond the input‚Äîwhile time complexity tracks operations.

For example, reversing an array in-place is O(1) space but creating a new reversed array is O(n) space. Both might be O(n) time, but the space requirements differ dramatically. This matters in memory-constrained environments like embedded systems or when processing massive datasets."

### Q2: Do you count the input in space complexity analysis?

**Perfect Answer:**

"No, we typically analyze auxiliary space‚Äîthe extra memory used beyond the input. The input is given to us and unavoidable, so we focus on additional allocations.

For example, if I receive an array of size n and create a hash set that could store all n elements, that's O(n) auxiliary space. The input array itself doesn't count.

However, I always clarify this with the interviewer because some contexts do count total space. Also, if the problem asks me to return a new structure of size n, whether that counts as 'auxiliary' depends on the problem requirements."

### Q3: How do you analyze space complexity of recursive algorithms?

**Perfect Answer:**

"For recursive algorithms, space complexity depends on the maximum call stack depth plus any auxiliary data structures used.

For example, binary search has O(log n) space recursively because we make at most log n recursive calls, each adding one stack frame. The iterative version is O(1) space.

Fibonacci is interesting: the naive recursive version is O(2^n) time but only O(n) space, because even though we make exponential calls, the maximum stack depth is just n‚Äîwe never go deeper than fib(n) ‚Üí fib(n-1) ‚Üí ... ‚Üí fib(1).

The formula is: Space = max(call stack depth, auxiliary structures used)."

### Q4: What's an in-place algorithm?

**Perfect Answer:**

"An in-place algorithm modifies the input data structure directly using only O(1) auxiliary space. It doesn't create proportionally-sized copies of the data.

For example, reversing an array by swapping elements from both ends is in-place‚Äîwe use two pointer variables (O(1)) and modify the original array. Creating a new reversed array would be O(n) space.

The trade-off is that in-place algorithms mutate the input, which might not be acceptable if we need to preserve the original data. I always ask the interviewer if mutating the input is allowed before proposing an in-place solution."

### Q5: Can you give an example of space-time tradeoff?

**Perfect Answer:**

"A classic example is checking for duplicates. The naive approach uses two nested loops: O(n¬≤) time but O(1) space. The optimized approach uses a hash set: O(n) time but O(n) space.

```python
# O(n¬≤) time, O(1) space
for i in range(len(arr)):
    for j in range(i+1, len(arr)):
        if arr[i] == arr[j]:
            return True

# O(n) time, O(n) space
seen = set()
for num in arr:
    if num in seen:
        return True
    seen.add(num)
```

We traded O(n) extra space for a massive time improvement. In practice, I'd use the hash set approach unless memory is severely constrained, because O(n) space is reasonable but O(n¬≤) time fails at scale."

### Q6: How does string manipulation affect space complexity?

**Perfect Answer:**

"Strings are immutable in most languages, so operations that seem in-place actually create new strings, using O(n) space.

For example, concatenating in a loop:

```python
result = ""
for char in s:
    result += char  # Each += creates NEW string!
```

This is actually O(n¬≤) space AND time because each concatenation creates a new string, and we do this n times.

The solution is using a list and joining:

```python
chars = []
for char in s:
    chars.append(char)  # O(1) amortized
result = ''.join(chars)  # O(n) at end
```

Now it's O(n) space total. String immutability is a common source of hidden space complexity."

### Q7: What's the space complexity of merge sort vs quick sort?

**Perfect Answer:**

"Merge sort is O(n) space because at each level of recursion, we create temporary arrays for merging, and there are log n levels. Even though we only have one level active at a time, the merging step itself requires O(n) auxiliary space.

Quick sort is O(log n) space on average‚Äîjust the call stack depth for balanced partitions. In the worst case (already sorted array with bad pivot choice), it degrades to O(n) space due to n-deep recursion.

This is why quick sort is often preferred despite having the same O(n log n) time complexity: it uses significantly less space. However, heap sort is even better at O(1) auxiliary space, though it's less cache-friendly in practice."

### Q8: How do you optimize space complexity in dynamic programming?

**Perfect Answer:**

"Many DP problems seem to require a 2D table, but can be optimized using the 'rolling array' technique if we only need the previous row or column.

For example, the Longest Common Subsequence problem:

```python
# Naive: O(m √ó n) space
dp = [[0] * (n+1) for _ in range(m+1)]

# Optimized: O(n) space (rolling array)
prev = [0] * (n+1)
curr = [0] * (n+1)
for i in range(1, m+1):
    for j in range(1, n+1):
        # ... compute curr[j] using prev
    prev, curr = curr, prev  # Swap
```

We reduced from O(m√ón) to O(n) by realizing we only ever need two rows at once. This pattern applies to many DP problems. The key is analyzing dependencies: if row i only depends on row i-1, we don't need the entire table."

---

## üîë Must Know Checklist

### ‚úÖ Critical (Master These First)

- ‚úÖ Understand auxiliary space vs input space
- ‚úÖ Can identify O(1), O(n), O(n¬≤) space
- ‚úÖ Know recursion uses stack space
- ‚úÖ Understand in-place algorithms
- ‚úÖ Can analyze simple iterative algorithms
- ‚úÖ Know space-time tradeoff concept
- ‚úÖ Understand why strings are O(n) to copy

### ‚úÖ Should Know

- ‚úÖ Can analyze recursive space complexity
- ‚úÖ Know rolling array optimization for DP
- ‚úÖ Understand when output space counts
- ‚úÖ Can compare space of different approaches
- ‚úÖ Know tail recursion concept
- ‚úÖ Understand stack overflow from deep recursion

### ‚úÖ Nice to Know (Advanced)

- ‚òê Understand amortized space analysis
- ‚òê Know memory allocation patterns
- ‚òê Familiar with cache locality impact
- ‚òê Understand garbage collection implications
- ‚òê Know language-specific optimizations

---

## üö® Common Mistakes to Avoid

### Mistake 1: Counting Input Space

```python
# ‚ùå WRONG: "This uses O(n) space because of the array"
def process(arr):
    return sum(arr)

# ‚úÖ CORRECT: "This uses O(1) auxiliary space"
# The input array doesn't count!
```

### Mistake 2: Forgetting String Immutability

```python
# ‚ùå WRONG: "This is O(1) space"
def build_string(n):
    result = ""
    for i in range(n):
        result += str(i)  # Creates NEW string each time!
    return result

# ‚úÖ CORRECT: "This is O(n¬≤) space due to string concatenation"
# Use list + join for O(n) space
```

### Mistake 3: Ignoring Recursion Stack

```python
# ‚ùå WRONG: "This is O(1) space because no new data structures"
def recursive_sum(arr, index=0):
    if index >= len(arr):
        return 0
    return arr[index] + recursive_sum(arr, index + 1)

# ‚úÖ CORRECT: "This is O(n) space due to call stack"
# n recursive calls = n stack frames
```

### Mistake 4: Not Clarifying Output Space

```python
# ‚ùå WRONG: Assuming output doesn't count
def get_all_subsets(arr):
    # Returns 2^n subsets...
    pass

# ‚úÖ CORRECT: Ask interviewer!
# "Should I count the output space? The output itself is 2^n elements."
```

### Mistake 5: Confusing Slicing with Indexing

```python
# ‚ùå WRONG: "Both are O(1) space"
element = arr[5]      # O(1) - just accessing
slice = arr[0:100]    # O(100) - creates NEW array!

# ‚úÖ CORRECT: Slicing creates copies = O(k) space
```

---

## üí° Pro Tips for Interviews

1. Always state both time AND space complexity for your solution
2. Clarify if output space counts - different companies have different standards
3. Mention the space-time tradeoff - shows you think holistically
4. Ask if you can modify input before proposing in-place solutions
5. Consider iterative vs recursive - mention space differences
6. Watch for hidden space usage - string concatenation, slicing, etc.
7. Draw the call stack for complex recursive problems
8. Propose optimizations - "This is O(n) space but we could optimize to O(1) by..."

---

## üìö Quick Reference Card

| Complexity | Name         | Example                 | When It Occurs            |
| ---------- | ------------ | ----------------------- | ------------------------- |
| O(1)       | Constant     | Variables, pointers     | Fixed variables, in-place |
| O(log n)   | Logarithmic  | Binary search recursion | Recursive halving         |
| O(n)       | Linear       | Hash set, new array     | Proportional structures   |
| O(n log n) | Linearithmic | Merge sort              | Divide-and-conquer        |
| O(n¬≤)      | Quadratic    | 2D matrix               | Grid/table storage        |

---

## üéØ Practice Problems by Space Complexity

### O(1) Space Practice

1. Reverse array in-place
2. Two-sum with sorted array (two pointers)
3. Remove duplicates from sorted array
4. Find max in array
5. Rotate array

### O(n) Space Practice

1. Two-sum with hash map
2. Group anagrams
3. Clone linked list with random pointer
4. Copy array
5. Build frequency map

### O(log n) Space Practice

1. Binary search (recursive)
2. Balanced BST operations
3. Merge sort (call stack only)
4. Find in rotated sorted array

### Space Optimization Practice

1. Fibonacci (optimize from O(n) to O(1))
2. LCS (optimize from O(m√ón) to O(n))
3. Unique paths (optimize DP table)
4. String to integer (watch for string creation)

---

## üéì Final Wisdom

> "Space complexity is often the difference between a solution that works on paper and one that works in production. A technically correct O(2^n) space algorithm will crash on real data. Always consider: can I do this in-place? Can I use a rolling array? Do I really need this hash map? Your future self (and your users) will thank you."

### Next Steps:

1. Practice analyzing space complexity of your solutions
2. Implement both space-optimized and time-optimized versions
3. Time yourself explaining space complexity in mock interviews
4. Review space complexity of common data structures and algorithms

**Remember: In interviews, always mention space complexity even if not asked. It shows depth of understanding!** O(1)

````

**Example 2: Swapping Two Numbers**

**Python:**
```python
def swap(arr, i, j):
    temp = arr[i]  # O(1) - one temporary variable
    arr[i] = arr[j]
    arr[j] = temp

# Fixed number of variables, independent of array size
# Space: O(1)
````

**JavaScript:**

```javascript
function swap(arr, i, j) {
  const temp = arr[i]; // O(1) - one temporary variable
  arr[i] = arr[j];
  arr[j] = temp;
}

// Fixed number of variables, independent of array size
// Space: O(1)
```

**Example 3: Two Pointers Technique**

**Python:**

```python
def reverse_array_in_place(arr):
    left = 0              # O(1)
    right = len(arr) - 1  # O(1)

    while left < right:
        arr[left], arr[right] = arr[right], arr[left]
        left += 1
        right -= 1

    return arr

# Only two pointer variables
# Modifying original array (in-place)
# Space: O(1)
```

**JavaScript:**

```javascript
function reverseArrayInPlace(arr) {
  let left = 0; // O(1)
  let right = arr.length - 1; // O(1)

  while (left < right) {
    [arr[left], arr[right]] = [arr[right], arr[left]];
    left++;
    right--;
  }

  return arr;
}

// Only two pointer variables
// Modifying original array (in-place)
// Space: O(1)
```

**Example 4: Iterative Sum**

**Python:**

```python
def sum_array(arr):
    total = 0  # O(1) - single accumulator

    for num in arr:
        total += num

    return total

# One variable regardless of array size
# Space: O(1)
```

**JavaScript:**

```javascript
function sumArray(arr) {
  let total = 0; // O(1) - single accumulator

  for (const num of arr) {
    total += num;
  }

  return total;
}

// One variable regardless of array size
// Space: O(1)
```

### Key Understanding Points:

- ‚úÖ Fixed number of variables
- ‚úÖ In-place algorithms (modify original data)
- ‚úÖ Iterative algorithms with few variables
- ‚úÖ Mathematical calculations
- üéØ **"In-place" = O(1) space**

### Interview Gold:

> "O(1) space means we use a constant, bounded amount of extra memory. Even if I use 100 variables, as long as that number doesn't grow with input size, it's still O(1). The key is **independence from input size**."

---

## 2Ô∏è‚É£ O(log n) - Logarithmic Space

### What it means:

**Memory grows logarithmically with input size**

This typically appears in recursive algorithms that halve the problem.

### Real-World Examples:

**Example 1: Binary Search (Recursive)**

**Python:**

```python
def binary_search_recursive(arr, target, left, right):
    if left > right:
        return -1

    mid = (left + right) // 2

    if arr[mid] == target:
        return mid
    elif arr[mid] < target:
        return binary_search_recursive(arr, target, mid + 1, right)
    else:
        return binary_search_recursive(arr, target, left, mid - 1)

# Call stack depth = log n
# Each recursive call stores: left, right, mid
# Maximum depth: log‚ÇÇ(n)
# Space: O(log n)
```

**JavaScript:**

```javascript
function binarySearchRecursive(arr, target, left, right) {
  if (left > right) {
    return -1;
  }

  const mid = Math.floor((left + right) / 2);

  if (arr[mid] === target) {
    return mid;
  } else if (arr[mid] < target) {
    return binarySearchRecursive(arr, target, mid + 1, right);
  } else {
    return binarySearchRecursive(arr, target, left, mid - 1);
  }
}

// Call stack depth = log n
// Each recursive call stores: left, right, mid
// Maximum depth: log‚ÇÇ(n)
// Space: O(log n)
```

**Visual Call Stack:**

```
Array size: 16 elements
binary_search(0, 15)      ‚Üê Level 1
  ‚Üí binary_search(0, 7)   ‚Üê Level 2
      ‚Üí binary_search(0, 3) ‚Üê Level 3
          ‚Üí binary_search(0, 1) ‚Üê Level 4
              ‚Üí binary_search(0, 0) ‚Üê Level 5

Maximum stack depth: log‚ÇÇ(16) = 4 levels
```

**Example 2: Finding Element in Balanced BST**

**Python:**

```python
class TreeNode:
    def __init__(self, val):
        self.val = val
        self.left = None
        self.right = None

def search_bst(root, target):
    if not root or root.val == target:
        return root

    if target < root.val:
        return search_bst(root.left, target)
    else:
        return search_bst(root.right, target)

# Balanced BST height = log n
# Recursion depth = height = log n
# Space: O(log n) for call stack
```

**JavaScript:**

```javascript
class TreeNode {
  constructor(val) {
    this.val = val;
    this.left = null;
    this.right = null;
  }
}

function searchBST(root, target) {
  if (!root || root.val === target) {
    return root;
  }

  if (target < root.val) {
    return searchBST(root.left, target);
  } else {
    return searchBST(root.right, target);
  }
}

// Balanced BST height = log n
// Recursion depth = height = log n
// Space: O(log n) for call stack
```

**Example 3: Merge Sort (Space for Recursion)**

**Python:**

```python
def merge_sort(arr):
    if len(arr) <= 1:
        return arr

    mid = len(arr) // 2
    left = merge_sort(arr[:mid])   # Recursive call
    right = merge_sort(arr[mid:])  # Recursive call

    return merge(left, right)

# Recursion depth: log n levels
# At each level: O(n) space for merging
# Total space: O(n) for arrays + O(log n) for call stack
#
# BUT if we count ONLY call stack: O(log n)
# Full analysis: O(n) due to temporary arrays
```

**JavaScript:**

```javascript
function mergeSort(arr) {
  if (arr.length <= 1) {
    return arr;
  }

  const mid = Math.floor(arr.length / 2);
  const left = mergeSort(arr.slice(0, mid)); // Recursive call
  const right = mergeSort(arr.slice(mid)); // Recursive call

  return merge(left, right);
}

// Recursion depth: log n levels
// At each level: O(n) space for merging
// Total space: O(n) for arrays + O(log n) for call stack
//
// BUT if we count ONLY call stack: O(log n)
// Full analysis: O(n) due to temporary arrays
```

### Key Understanding Points:

- ‚úÖ Recursive divide-and-conquer (call stack)
- ‚úÖ Balanced tree operations
- ‚úÖ Algorithms that halve the problem
- ‚úÖ Call stack depth = space used
- üéØ **Each recursive call adds one stack frame**

### Interview Gold:

> "Logarithmic space usually comes from recursive algorithms that divide the problem in half. Each recursive call uses stack space, and if we divide by 2 each time, we get log n depth. That's why recursive binary search is O(log n) space, but iterative binary search is O(1) space."

---

## 3Ô∏è‚É£ O(n) - Linear Space

### What it means:

**Memory grows proportionally with input size**

### Real-World Examples:

**Example 1: Creating a Copy of an Array**

**Python:**

```python
def copy_array(arr):
    result = []  # New array

    for item in arr:
        result.append(item)

    return result

# New array with n elements
# Space: O(n)
```

**JavaScript:**

```javascript
function copyArray(arr) {
    const result = [];  # New array

    for (const item of arr) {
        result.push(item);
    }

    return result;
}

// New array with n elements
// Space: O(n)
```

**Example 2: Hash Set for Unique Elements**

**Python:**

```python
def remove_duplicates(arr):
    seen = set()  # O(n) space - could store all elements
    result = []

    for num in arr:
        if num not in seen:
            seen.add(num)
            result.append(num)

    return result

# Worst case: all elements unique
# seen stores n elements
# Space: O(n)
```

**JavaScript:**

```javascript
function removeDuplicates(arr) {
  const seen = new Set(); // O(n) space - could store all elements
  const result = [];

  for (const num of arr) {
    if (!seen.has(num)) {
      seen.add(num);
      result.push(num);
    }
  }

  return result;
}

// Worst case: all elements unique
// seen stores n elements
// Space: O(n)
```

**Example 3: Hash Map for Frequency Count**

**Python:**

```python
def character_frequency(s):
    freq = {}  # O(n) space in worst case

    for char in s:
        freq[char] = freq.get(char, 0) + 1

    return freq

# If all characters unique: n key-value pairs
# Space: O(n)
#
# Note: If limited alphabet (e.g., lowercase letters),
# could argue O(1) since max 26 keys
```

**JavaScript:**

```javascript
function characterFrequency(s) {
  const freq = {}; // O(n) space in worst case

  for (const char of s) {
    freq[char] = (freq[char] || 0) + 1;
  }

  return freq;
}

// If all characters unique: n key-value pairs
// Space: O(n)
//
// Note: If limited alphabet (e.g., lowercase letters),
// could argue O(1) since max 26 keys
```

**Example 4: Recursive Fibonacci (Call Stack)**

**Python:**

```python
def fibonacci_recursive(n):
    if n <= 1:
        return n

    return fibonacci_recursive(n - 1) + fibonacci_recursive(n - 2)

# Maximum call stack depth: n
# Each call stores: n, return address
# Space: O(n) for call stack
#
# Time is O(2‚Åø) but space is O(n)!
```

**JavaScript:**

```javascript
function fibonacciRecursive(n) {
  if (n <= 1) {
    return n;
  }

  return fibonacciRecursive(n - 1) + fibonacciRecursive(n - 2);
}

// Maximum call stack depth: n
// Each call stores: n, return address
// Space: O(n) for call stack
//
// Time is O(2‚Åø) but space is O(n)!
```

**Call Stack Visualization:**

```
fib(5)
  ‚Üí fib(4)
      ‚Üí fib(3)
          ‚Üí fib(2)
              ‚Üí fib(1)  ‚Üê Maximum depth: 5 levels

Even though we make exponential calls,
we never go deeper than n in the stack!
```

**Example 5: Building Result Array**

**Python:**

```python
def squares(arr):
    result = []  # New array

    for num in arr:
        result.append(num * num)

    return result

# Creating new array of size n
# Space: O(n)
```

**JavaScript:**

```javascript
function squares(arr) {
  const result = []; // New array

  for (const num of arr) {
    result.push(num * num);
  }

  return result;
}

// Creating new array of size n
// Space: O(n)
```

**Example 6: String Builder / Array Join**

**Python:**

```python
def reverse_words(s):
    words = s.split()  # O(n) space - new list
    reversed_words = words[::-1]  # O(n) space - another new list
    return ' '.join(reversed_words)  # O(n) space - new string

# Total: O(n) space
# (Constants don't matter: O(3n) = O(n))
```

**JavaScript:**

```javascript
function reverseWords(s) {
  const words = s.split(" "); // O(n) space - new array
  const reversedWords = words.reverse(); // O(1) - reverses in place
  return reversedWords.join(" "); // O(n) space - new string
}

// Total: O(n) space
```

### Key Understanding Points:

- ‚úÖ Creating new arrays/lists of input size
- ‚úÖ Hash tables that could store all elements
- ‚úÖ Recursive call stacks of depth n
- ‚úÖ String manipulations (strings are immutable)
- ‚úÖ Multiple O(n) structures still equals O(n)
- üéØ **Most common auxiliary space complexity**

### Common Hidden O(n) Space:

**Python:**

```python
# These all use O(n) space!

# 1. String slicing
substring = s[0:n]  # Creates new string: O(n)

# 2. List slicing
sublist = arr[0:n]  # Creates new list: O(n)

# 3. List comprehension
squared = [x*x for x in arr]  # New list: O(n)

# 4. String concatenation in loop
result = ""
for char in s:
    result += char  # Each += creates new string: O(n¬≤) total!

# 5. Sorting (often uses extra space)
sorted_arr = sorted(arr)  # Timsort uses O(n): O(n)
```

**JavaScript:**

```javascript
// These all use O(n) space!

// 1. String slicing
const substring = s.slice(0, n); // Creates new string: O(n)

// 2. Array slicing
const subarray = arr.slice(0, n); // Creates new array: O(n)

// 3. Array map/filter
const squared = arr.map((x) => x * x); // New array: O(n)

// 4. String concatenation in loop
let result = "";
for (const char of s) {
  result += char; // Each += creates new string: O(n¬≤) total!
}

// 5. Spread operator
const copy = [...arr]; // Creates new array: O(n)

// 6. Array.from()
const newArr = Array.from(arr); // Creates new array: O(n)
```

### Interview Gold:

> "O(n) space is the most common auxiliary space complexity. Whenever I create a new data structure proportional to input size‚Äîlike a hash map, array, or set‚ÄîI'm using O(n) space. The key insight is that 3 separate O(n) structures still equals O(n) total, not O(3n), because we drop constants."

---

## 4Ô∏è‚É£ O(n¬≤) - Quadratic Space

### What it means:

**Memory grows quadratically with input size**

This is rare and usually indicates a problem that needs optimization.

### Real-World Examples:

**Example 1: 2D Matrix/Grid**

**Python:**

```python
def create_multiplication_table(n):
    table = []

    for i in range(1, n + 1):
        row = []
        for j in range(1, n + 1):
            row.append(i * j)
        table.append(row)

    return table

# n √ó n matrix
# n rows, each with n elements
# Space: O(n¬≤)

# Example: n=5
# [
#   [1,  2,  3,  4,  5],
#   [2,  4,  6,  8, 10],
#   [3,  6,  9, 12, 15],
#   [4,  8, 12, 16, 20],
#   [5, 10, 15, 20, 25]
# ]
# Total elements: 5 √ó 5 = 25 = n¬≤
```

**JavaScript:**

```javascript
function createMultiplicationTable(n) {
  const table = [];

  for (let i = 1; i <= n; i++) {
    const row = [];
    for (let j = 1; j <= n; j++) {
      row.push(i * j);
    }
    table.push(row);
  }

  return table;
}

// n √ó n matrix
// n rows, each with n elements
// Space: O(n¬≤)

// Example: n=5
// [
//   [1,  2,  3,  4,  5],
//   [2,  4,  6,  8, 10],
//   [3,  6,  9, 12, 15],
//   [4,  8, 12, 16, 20],
//   [5, 10, 15, 20, 25]
// ]
// Total elements: 5 √ó 5 = 25 = n¬≤
```

**Example 2: Graph as Adjacency Matrix**

**Python:**

```python
def create_adjacency_matrix(n):
    # Create n √ó n matrix for graph with n vertices
    matrix = [[0] * n for _ in range(n)]

    return matrix

# Space: O(n¬≤)
# Each of n vertices can connect to n vertices
# Total cells: n √ó n = n¬≤
```

**JavaScript:**

```javascript
function createAdjacencyMatrix(n) {
  // Create n √ó n matrix for graph with n vertices
  const matrix = Array.from({ length: n }, () => Array(n).fill(0));

  return matrix;
}

// Space: O(n¬≤)
// Each of n vertices can connect to n vertices
// Total cells: n √ó n = n¬≤
```

**Example 3: Dynamic Programming Table**

**Python:**

```python
def longest_common_subsequence(text1, text2):
    m, n = len(text1), len(text2)

    # Create (m+1) √ó (n+1) DP table
    dp = [[0] * (n + 1) for _ in range(m + 1)]

    for i in range(1, m + 1):
        for j in range(1, n + 1):
            if text1[i-1] == text2[j-1]:
                dp[i][j] = dp[i-1][j-1] + 1
            else:
                dp[i][j] = max(dp[i-1][j], dp[i][j-1])

    return dp[m][n]

# If both strings length n: dp table is n √ó n
# Space: O(n¬≤)
#
# Note: Can optimize to O(n) with rolling array technique
```

**JavaScript:**

```javascript
function longestCommonSubsequence(text1, text2) {
  const m = text1.length,
    n = text2.length;

  // Create (m+1) √ó (n+1) DP table
  const dp = Array.from({ length: m + 1 }, () => Array(n + 1).fill(0));

  for (let i = 1; i <= m; i++) {
    for (let j = 1; j <= n; j++) {
      if (text1[i - 1] === text2[j - 1]) {
        dp[i][j] = dp[i - 1][j - 1] + 1;
      } else {
        dp[i][j] = Math.max(dp[i - 1][j], dp[i][j - 1]);
      }
    }
  }

  return dp[m][n];
}

// If both strings length n: dp table is n √ó n
// Space: O(n¬≤)
//
// Note: Can optimize to O(n) with rolling array technique
```

**Example 4: Storing All Pairs**

**Python:**

```python
def all_pairs(arr):
    pairs = []

    for i in range(len(arr)):
        for j in range(len(arr)):
            pairs.append((arr[i], arr[j]))

    return pairs

# n elements ‚Üí n¬≤ pairs
# Space: O(n¬≤)
#
# For n=100: 10,000 pairs stored!
```

**JavaScript:**

```javascript
function allPairs(arr) {
  const pairs = [];

  for (let i = 0; i < arr.length; i++) {
    for (let j = 0; j < arr.length; j++) {
      pairs.push([arr[i], arr[j]]);
    }
  }

  return pairs;
}

// n elements ‚Üí n¬≤ pairs
// Space: O(n¬≤)
//
// For n=100: 10,000 pairs stored!
```

### Key Understanding Points:

- ‚úÖ 2D arrays/matrices
- ‚úÖ Graph adjacency matrices
- ‚úÖ Some DP tables (can often optimize)
- ‚úÖ Storing all pairs/combinations
- ‚ö†Ô∏è Usually too much space‚Äîlook for optimizations
- üéØ Red flag: can we reduce to O(n)?

### Common Optimization:

**Python:**

```python
# ‚ùå BAD: O(n¬≤) space
def lcs_space_heavy(text1, text2):
    m, n = len(text1), len(text2)
    dp = [[0] * (n + 1) for _ in range(m + 1)]
    # ... fill table
    return dp[m][n]

# ‚úÖ GOOD: O(n) space (rolling array)
def lcs_space_optimized(text1, text2):
    m, n = len(text1), len(text2)
    # Only need current and previous row
    prev = [0] * (n + 1)
    curr = [0] * (n + 1)

    for i in range(1, m + 1):
        for j in range(1, n + 1):
            if text1[i-1] == text2[j-1]:
                curr[j] = prev[j-1] + 1
            else:
                curr[j] = max(prev[j], curr[j-1])
        prev, curr = curr, prev  # Swap rows

    return prev[n]

# Reduced from O(m√ón) to O(n)!
```

**JavaScript:**

```javascript
// ‚ùå BAD: O(n¬≤) space
function lcsSpaceHeavy(text1, text2) {
  const m = text1.length,
    n = text2.length;
  const dp = Array.from({ length: m + 1 }, () => Array(n + 1).fill(0));
  // ... fill table
  return dp[m][n];
}

// ‚úÖ GOOD: O(n) space (rolling array)
function lcsSpaceOptimized(text1, text2) {
  const m = text1.length,
    n = text2.length;
  // Only need current and previous row
  let prev = Array(n + 1).fill(0);
  let curr = Array(n + 1).fill(0);

  for (let i = 1; i <= m; i++) {
    for (let j = 1; j <= n; j++) {
      if (text1[i - 1] === text2[j - 1]) {
        curr[j] = prev[j - 1] + 1;
      } else {
        curr[j] = Math.max(prev[j], curr[j - 1]);
      }
    }
    [prev, curr] = [curr, prev]; // Swap rows
  }

  return prev[n];
}

// Reduced from O(m√ón) to O(n)!
```

### Interview Gold:

> "Quadratic space is a red flag. When I see O(n¬≤) space in my solution, I immediately ask: do I really need the entire 2D table, or can I use a rolling array? Many DP problems that seem to need O(n¬≤) space can be optimized to O(n) by only keeping the rows or columns we actually need."

---

## üîÑ Recursion and Stack Space

### Understanding the Call Stack

Every recursive call consumes stack space!

**Python:**

```python
def factorial(n):
    if n <= 1:
        return 1
    return n * factorial(n - 1)

# Call stack for factorial(5):
# factorial(5)
#   ‚Üí factorial(4)
#       ‚Üí factorial(3)
#           ‚Üí factorial(2)
#               ‚Üí factorial(1)  ‚Üê Max depth: 5

# Space: O(n) for call stack
# Each call stores: n, return address, local variables
```

**JavaScript:**

```javascript
function factorial(n) {
  if (n <= 1) {
    return 1;
  }
  return n * factorial(n - 1);
}

// Call stack for factorial(5):
// factorial(5)
//   ‚Üí factorial(4)
//       ‚Üí factorial(3)
//           ‚Üí factorial(2)
//               ‚Üí factorial(1)  ‚Üê Max depth: 5

// Space:
```
