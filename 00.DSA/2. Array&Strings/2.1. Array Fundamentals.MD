# Array Fundamentals - Complete Deep Dive

## üéØ Key Concepts

### What Arrays Actually Are

**Arrays** are the most fundamental data structure‚Äîa contiguous block of memory storing elements of the same type, accessible by index.

**Think of it like this:**

- Array: "A row of numbered boxes in memory"
- Index: "The box number (starting at 0)"
- Element: "What's inside each box"
- Contiguous: "All boxes are right next to each other in memory"

---

## üìä Memory Representation

### How Arrays Live in Memory

```
Memory Address:  1000   1004   1008   1012   1016
Array:          [  5  |  12  |  7   |  23  |  9  ]
Index:             0      1      2      3      4
```

**Key Points:**

- Each element occupies fixed memory (e.g., 4 bytes for int)
- Elements stored consecutively
- Address calculation: `base_address + (index √ó element_size)`
- This is why array access is **O(1)**!

---

## üî¢ Static vs Dynamic Arrays

### Static Arrays

**Definition:** Fixed size allocated at creation

**Characteristics:**

<details open>
<summary><b>Python</b> (Note: Python doesn't have true static arrays natively)</summary>

```python
# Python lists are dynamic, but we can simulate static with array module
from array import array

# Create static array of integers
static_arr = array('i', [1, 2, 3, 4, 5])  # Fixed type, but can still resize

# True static arrays in Python require numpy
import numpy as np
static_np = np.array([1, 2, 3, 4, 5], dtype=np.int32)
```

</details>

<details>
<summary><b>JavaScript</b></summary>

```javascript
// JavaScript arrays are dynamic, but we can create fixed-length buffers
const buffer = new ArrayBuffer(20); // 20 bytes
const staticArr = new Int32Array(buffer); // 5 integers (4 bytes each)

staticArr[0] = 10;
staticArr[1] = 20;
// Length is fixed at 5

// Regular arrays in JS are actually dynamic
const dynamicArr = [1, 2, 3]; // Can grow/shrink
```

</details>

<details>
<summary><b>Java</b></summary>

```java
// True static array
int[] staticArr = new int[5];  // Fixed size of 5
staticArr[0] = 10;
staticArr[1] = 20;

// Cannot change size - attempting to add 6th element causes error
// staticArr[5] = 30;  // ArrayIndexOutOfBoundsException
```

</details>

<details>
<summary><b>C++</b></summary>

```cpp
// True static array
int staticArr[5];  // Fixed size of 5
staticArr[0] = 10;
staticArr[1] = 20;

// Size is compile-time constant
// sizeof(staticArr) = 5 * sizeof(int) = 20 bytes
```

</details>

**Pros:**

- ‚úÖ Simple and fast
- ‚úÖ No overhead for resizing
- ‚úÖ Memory allocated once
- ‚úÖ Predictable memory usage

**Cons:**

- ‚ùå Fixed size (can't grow)
- ‚ùå Wasted space if not fully used
- ‚ùå Need to know size in advance

---

### Dynamic Arrays

**Definition:** Can grow/shrink as needed

**How They Work:**

```
Initial: [1, 2, 3, 4]  (capacity: 4, size: 4)

Add element 5:
1. Check if full ‚Üí Yes!
2. Allocate new array (capacity √ó 2 = 8)
3. Copy all elements ‚Üí [1, 2, 3, 4, 5, _, _, _]
4. Delete old array

Result: [1, 2, 3, 4, 5]  (capacity: 8, size: 5)
```

<details open>
<summary><b>Python</b></summary>

```python
# Python lists are dynamic arrays
arr = []  # Empty dynamic array

# Append is amortized O(1)
arr.append(1)  # [1]
arr.append(2)  # [1, 2]
arr.append(3)  # [1, 2, 3]

# When capacity is reached, Python doubles the size
# Capacity growth: 0, 4, 8, 16, 32, 64, ...

# Insert at beginning is O(n) - must shift all elements
arr.insert(0, 0)  # [0, 1, 2, 3]

# Remove from end is O(1)
arr.pop()  # [0, 1, 2]

# Remove from beginning is O(n)
arr.pop(0)  # [1, 2]
```

</details>

<details>
<summary><b>JavaScript</b></summary>

```javascript
// JavaScript arrays are always dynamic
let arr = []; // Empty dynamic array

// Push is amortized O(1)
arr.push(1); // [1]
arr.push(2); // [1, 2]
arr.push(3); // [1, 2, 3]

// Unshift (add to beginning) is O(n)
arr.unshift(0); // [0, 1, 2, 3]

// Pop (remove from end) is O(1)
arr.pop(); // [0, 1, 2]

// Shift (remove from beginning) is O(n)
arr.shift(); // [1, 2]

// Arrays in JS can be sparse (not truly contiguous)
arr[10] = 100; // [1, 2, empty √ó 8, 100]
```

</details>

<details>
<summary><b>Java</b></summary>

```java
// ArrayList is Java's dynamic array
ArrayList<Integer> arr = new ArrayList<>();

// Add is amortized O(1)
arr.add(1);  // [1]
arr.add(2);  // [1, 2]
arr.add(3);  // [1, 2, 3]

// Default capacity: 10
// When full, increases by 50%: 10 ‚Üí 15 ‚Üí 22 ‚Üí 33 ...

// Add at index 0 is O(n)
arr.add(0, 0);  // [0, 1, 2, 3]

// Remove last is O(1)
arr.remove(arr.size() - 1);  // [0, 1, 2]

// Remove first is O(n)
arr.remove(0);  // [1, 2]
```

</details>

<details>
<summary><b>C++</b></summary>

```cpp
// std::vector is C++'s dynamic array
std::vector<int> arr;

// push_back is amortized O(1)
arr.push_back(1);  // [1]
arr.push_back(2);  // [1, 2]
arr.push_back(3);  // [1, 2, 3]

// Capacity doubles when full
// Reserve space to avoid reallocations
arr.reserve(100);  // Pre-allocate capacity

// Insert at beginning is O(n)
arr.insert(arr.begin(), 0);  // [0, 1, 2, 3]

// pop_back is O(1)
arr.pop_back();  // [0, 1, 2]

// erase from beginning is O(n)
arr.erase(arr.begin());  // [1, 2]
```

</details>

**Amortized O(1) Insertion:**

```
Operation | Actual Cost | Explanation
----------|-------------|------------
Add 1st   | 1           | Copy 0, insert 1
Add 2nd   | 2           | Resize (1‚Üí2), copy 1, insert 1
Add 3rd   | 1           | Space available, insert 1
Add 4th   | 4           | Resize (2‚Üí4), copy 2, insert 1
Add 5th   | 1           | Space available
Add 6th   | 1           | Space available
Add 7th   | 1           | Space available
Add 8th   | 8           | Resize (4‚Üí8), copy 4, insert 1

Total: 19 operations for 8 elements
Average: 19/8 ‚âà 2.4 operations per insertion
This averages to O(1) per operation!
```

**Pros:**

- ‚úÖ Flexible size
- ‚úÖ Efficient append (amortized O(1))
- ‚úÖ No wasted space (eventually)
- ‚úÖ Practical for most use cases

**Cons:**

- ‚ùå Occasional expensive resize (O(n))
- ‚ùå May waste space (capacity > size)
- ‚ùå Resizing requires copying all elements

---

## ‚ö° Array Operations & Complexity

### Complete Operation Reference

| Operation              | Description     | Time           | Space | Notes                     |
| ---------------------- | --------------- | -------------- | ----- | ------------------------- |
| **Access**             | `arr[i]`        | O(1)           | O(1)  | Direct memory calculation |
| **Search** (unsorted)  | Find element    | O(n)           | O(1)  | Must check each element   |
| **Search** (sorted)    | Binary search   | O(log n)       | O(1)  | Only if sorted            |
| **Insert** (end)       | Append          | O(1) amortized | O(1)  | May need resize           |
| **Insert** (beginning) | Prepend         | O(n)           | O(1)  | Shift all elements        |
| **Insert** (middle)    | Insert at index | O(n)           | O(1)  | Shift elements after      |
| **Delete** (end)       | Remove last     | O(1)           | O(1)  | Just decrease size        |
| **Delete** (beginning) | Remove first    | O(n)           | O(1)  | Shift all elements        |
| **Delete** (middle)    | Remove at index | O(n)           | O(1)  | Shift elements after      |
| **Update**             | `arr[i] = x`    | O(1)           | O(1)  | Direct access             |

---

### Access by Index - O(1)

**Why it's constant time:**

```python
# Mathematical formula:
address = base_address + (index √ó element_size)

# Example:
base = 1000
index = 3
element_size = 4 bytes

address = 1000 + (3 √ó 4) = 1012

# Single arithmetic operation ‚Üí O(1)
```

<details open>
<summary><b>Python</b></summary>

```python
arr = [10, 20, 30, 40, 50]

# Access: O(1)
first = arr[0]      # 10
third = arr[2]      # 30
last = arr[-1]      # 50 (Python supports negative indexing)

# Update: O(1)
arr[2] = 100        # [10, 20, 100, 40, 50]

# No matter if array has 10 or 10 million elements,
# accessing arr[5] takes same time!
```

</details>

<details>
<summary><b>JavaScript</b></summary>

```javascript
const arr = [10, 20, 30, 40, 50];

// Access: O(1)
const first = arr[0]; // 10
const third = arr[2]; // 30
const last = arr[arr.length - 1]; // 50

// Update: O(1)
arr[2] = 100; // [10, 20, 100, 40, 50]

// Arrays are objects, so you can do:
arr[100] = 200; // Creates sparse array
```

</details>

**Interview Gold:**

> "Array access is O(1) because we can calculate the exact memory address mathematically. It's just base_address + (index √ó element_size). This is why arrays are called 'random access' structures‚Äîany element is equally fast to reach."

---

### Insertion - Complexity Depends on Position

#### Insert at End - O(1) Amortized

<details open>
<summary><b>Python</b></summary>

```python
arr = [1, 2, 3]

# Append: O(1) amortized
arr.append(4)  # [1, 2, 3, 4]

# If capacity is exceeded, Python:
# 1. Allocates new array (double size)
# 2. Copies all elements ‚Üí O(n) occasionally
# 3. But averages to O(1) per operation

# 100 appends costs ~200 operations total
# 200/100 = 2 operations average = O(1)
```

</details>

<details>
<summary><b>JavaScript</b></summary>

```javascript
const arr = [1, 2, 3];

// Push: O(1) amortized
arr.push(4); // [1, 2, 3, 4]

// Can push multiple elements
arr.push(5, 6, 7); // [1, 2, 3, 4, 5, 6, 7]
```

</details>

#### Insert at Beginning - O(n)

<details open>
<summary><b>Python</b></summary>

```python
arr = [2, 3, 4, 5]

# Insert at beginning: O(n)
arr.insert(0, 1)  # [1, 2, 3, 4, 5]

# What happens internally:
# [2, 3, 4, 5]
#     ‚Üì shift right
# [_, 2, 3, 4, 5]
#  ‚Üì insert
# [1, 2, 3, 4, 5]

# Must shift n elements ‚Üí O(n)
```

</details>

<details>
<summary><b>JavaScript</b></summary>

```javascript
const arr = [2, 3, 4, 5];

// Unshift: O(n)
arr.unshift(1); // [1, 2, 3, 4, 5]

// Must shift all elements right
```

</details>

**Visual Representation:**

```
Before: [2][3][4][5]
         ‚Üì  ‚Üì  ‚Üì  ‚Üì   (all shift right)
After:  [_][2][3][4][5]
         ‚Üì   (insert 1)
Final:  [1][2][3][4][5]

Operations: n shifts + 1 insert = O(n)
```

#### Insert at Middle - O(n)

<details open>
<summary><b>Python</b></summary>

```python
arr = [1, 2, 4, 5]

# Insert at index 2: O(n)
arr.insert(2, 3)  # [1, 2, 3, 4, 5]

# Only elements AFTER index need to shift
# Average case: shift n/2 elements = O(n)
```

</details>

<details>
<summary><b>JavaScript</b></summary>

```javascript
const arr = [1, 2, 4, 5];

// Splice to insert: O(n)
arr.splice(2, 0, 3); // [1, 2, 3, 4, 5]
//         ^  ^  ^
//         |  |  value to insert
//         |  deleteCount (0 = insert only)
//         index
```

</details>

---

### Deletion - Complexity Depends on Position

#### Delete from End - O(1)

<details open>
<summary><b>Python</b></summary>

```python
arr = [1, 2, 3, 4, 5]

# Remove last: O(1)
last = arr.pop()  # Returns 5, arr = [1, 2, 3, 4]

# No shifting needed, just decrease size
```

</details>

<details>
<summary><b>JavaScript</b></summary>

```javascript
const arr = [1, 2, 3, 4, 5];

// Pop: O(1)
const last = arr.pop(); // Returns 5, arr = [1, 2, 3, 4]
```

</details>

#### Delete from Beginning - O(n)

<details open>
<summary><b>Python</b></summary>

```python
arr = [1, 2, 3, 4, 5]

# Remove first: O(n)
first = arr.pop(0)  # Returns 1, arr = [2, 3, 4, 5]

# What happens:
# [1][2][3][4][5]
#     ‚Üì  ‚Üì  ‚Üì  ‚Üì  (all shift left)
# [2][3][4][5]

# Must shift n-1 elements ‚Üí O(n)
```

</details>

<details>
<summary><b>JavaScript</b></summary>

```javascript
const arr = [1, 2, 3, 4, 5];

// Shift: O(n)
const first = arr.shift(); // Returns 1, arr = [2, 3, 4, 5]
```

</details>

#### Delete from Middle - O(n)

<details open>
<summary><b>Python</b></summary>

```python
arr = [1, 2, 3, 4, 5]

# Delete by index: O(n)
del arr[2]  # arr = [1, 2, 4, 5]

# Or using pop:
arr = [1, 2, 3, 4, 5]
arr.pop(2)  # Returns 3, arr = [1, 2, 4, 5]

# Or using remove (searches then deletes):
arr = [1, 2, 3, 4, 5]
arr.remove(3)  # arr = [1, 2, 4, 5]  (O(n) search + O(n) shift)
```

</details>

<details>
<summary><b>JavaScript</b></summary>

```javascript
const arr = [1, 2, 3, 4, 5];

// Splice to delete: O(n)
arr.splice(2, 1); // Delete 1 element at index 2
// arr = [1, 2, 4, 5]

// Delete operator (not recommended for arrays)
delete arr[2]; // arr = [1, 2, empty, 4, 5] (creates sparse array)
```

</details>

---

### Search Operations

#### Linear Search - O(n)

<details open>
<summary><b>Python</b></summary>

```python
def linear_search(arr, target):
    """Search unsorted array"""
    for i in range(len(arr)):
        if arr[i] == target:
            return i  # Found at index i
    return -1  # Not found

arr = [5, 2, 8, 1, 9]
index = linear_search(arr, 8)  # Returns 2

# Must potentially check all n elements ‚Üí O(n)

# Python's built-in:
if 8 in arr:  # O(n) - linear search
    index = arr.index(8)  # O(n) - linear search
```

</details>

<details>
<summary><b>JavaScript</b></summary>

```javascript
function linearSearch(arr, target) {
  for (let i = 0; i < arr.length; i++) {
    if (arr[i] === target) {
      return i; // Found at index i
    }
  }
  return -1; // Not found
}

const arr = [5, 2, 8, 1, 9];
const index = linearSearch(arr, 8); // Returns 2

// Built-in methods:
arr.includes(8); // true, O(n)
arr.indexOf(8); // 2, O(n)
arr.find((x) => x === 8); // 8, O(n)
arr.findIndex((x) => x === 8); // 2, O(n)
```

</details>

#### Binary Search - O(log n)

**Requires: SORTED array**

<details open>
<summary><b>Python</b></summary>

```python
def binary_search(arr, target):
    """Search sorted array - O(log n)"""
    left, right = 0, len(arr) - 1

    while left <= right:
        mid = (left + right) // 2

        if arr[mid] == target:
            return mid
        elif arr[mid] < target:
            left = mid + 1  # Search right half
        else:
            right = mid - 1  # Search left half

    return -1

# Sorted array required!
arr = [1, 2, 5, 8, 9]
index = binary_search(arr, 8)  # Returns 3

# With each comparison, eliminate half the array
# n ‚Üí n/2 ‚Üí n/4 ‚Üí ... ‚Üí 1
# Takes log‚ÇÇ(n) steps
```

</details>

<details>
<summary><b>JavaScript</b></summary>

```javascript
function binarySearch(arr, target) {
  let left = 0;
  let right = arr.length - 1;

  while (left <= right) {
    const mid = Math.floor((left + right) / 2);

    if (arr[mid] === target) {
      return mid;
    } else if (arr[mid] < target) {
      left = mid + 1; // Search right half
    } else {
      right = mid - 1; // Search left half
    }
  }

  return -1;
}

// Sorted array required!
const arr = [1, 2, 5, 8, 9];
const index = binarySearch(arr, 8); // Returns 3
```

</details>

**Interview Gold:**

> "The key difference: linear search works on any array and is O(n). Binary search requires a sorted array but is O(log n). If I need to search multiple times, it's worth sorting first (O(n log n)) then doing binary searches (O(log n) each)."

---

## üß† Multi-Dimensional Arrays

### 2D Arrays (Matrices)

<details open>
<summary><b>Python</b></summary>

```python
# Create 2D array (matrix)
matrix = [
    [1, 2, 3],
    [4, 5, 6],
    [7, 8, 9]
]

# Access: O(1)
element = matrix[1][2]  # 6 (row 1, col 2)

# Create with list comprehension
rows, cols = 3, 4
matrix = [[0 for _ in range(cols)] for _ in range(rows)]
# [[0,0,0,0],
#  [0,0,0,0],
#  [0,0,0,0]]

# WRONG way (common mistake):
wrong = [[0] * cols] * rows  # All rows reference SAME list!
wrong[0][0] = 1
# [[1,0,0,0],
#  [1,0,0,0],  ‚Üê All changed!
#  [1,0,0,0]]
```

</details>

<details>
<summary><b>JavaScript</b></summary>

```javascript
// Create 2D array
const matrix = [
  [1, 2, 3],
  [4, 5, 6],
  [7, 8, 9],
];

// Access: O(1)
const element = matrix[1][2]; // 6

// Create with Array.from
const rows = 3,
  cols = 4;
const matrix2 = Array.from({ length: rows }, () => Array(cols).fill(0));
// [[0,0,0,0],
//  [0,0,0,0],
//  [0,0,0,0]]

// Or with map
const matrix3 = Array(rows)
  .fill()
  .map(() => Array(cols).fill(0));
```

</details>

### Memory Layout (Row-Major Order)

```
2D Array:       Memory (row-major):
[1, 2, 3]       [1][2][3][4][5][6][7][8][9]
[4, 5, 6]        ‚Üë     ‚Üë     ‚Üë
[7, 8, 9]      row 0  row 1  row 2

Address of element [i][j]:
base + (i √ó num_cols + j) √ó element_size
```

### Common 2D Array Operations

<details open>
<summary><b>Python</b></summary>

```python
matrix = [
    [1, 2, 3],
    [4, 5, 6],
    [7, 8, 9]
]

# Traverse all elements: O(rows √ó cols)
for i in range(len(matrix)):
    for j in range(len(matrix[0])):
        print(matrix[i][j])

# Get dimensions
rows = len(matrix)      # 3
cols = len(matrix[0])   # 3 (assuming rectangular)

# Transpose: O(rows √ó cols)
transposed = [[matrix[j][i] for j in range(rows)]
              for i in range(cols)]
# [[1, 4, 7],
#  [2, 5, 8],
#  [3, 6, 9]]

# Rotate 90¬∞ clockwise: O(n¬≤)
def rotate_90(matrix):
    n = len(matrix)
    # Transpose
    for i in range(n):
        for j in range(i + 1, n):
            matrix[i][j], matrix[j][i] = matrix[j][i], matrix[i][j]
    # Reverse each row
    for i in range(n):
        matrix[i].reverse()
    return matrix
```

</details>

<details>
<summary><b>JavaScript</b></summary>

```javascript
const matrix = [
  [1, 2, 3],
  [4, 5, 6],
  [7, 8, 9],
];

// Traverse all elements: O(rows √ó cols)
for (let i = 0; i < matrix.length; i++) {
  for (let j = 0; j < matrix[0].length; j++) {
    console.log(matrix[i][j]);
  }
}

// Get dimensions
const rows = matrix.length; // 3
const cols = matrix[0].length; // 3

// Transpose: O(rows √ó cols)
const transposed = matrix[0].map((_, colIndex) =>
  matrix.map((row) => row[colIndex])
);
// [[1, 4, 7],
//  [2, 5, 8],
//  [3, 6, 9]]

// Rotate 90¬∞ clockwise: O(n¬≤)
function rotate90(matrix) {
  const n = matrix.length;
  // Transpose
  for (let i = 0; i < n; i++) {
    for (let j = i + 1; j < n; j++) {
      [matrix[i][j], matrix[j][i]] = [matrix[j][i], matrix[i][j]];
    }
  }
  // Reverse each row
  matrix.forEach((row) => row.reverse());
  return matrix;
}
```

</details>

---

## üéØ Array Traversal Techniques

### Forward Traversal

<details open>
<summary><b>Python</b></summary>

```python
arr = [1, 2, 3, 4, 5]

# Method 1: Index-based
for i in range(len(arr)):
    print(f"Index {i}: {arr[i]}")

# Method 2: Direct iteration
for num in arr:
    print(num)

# Method 3: With enumerate (index + value)
for i, num in enumerate(arr):
    print(f"Index {i}: {num}")
```

</details>

<details>
<summary><b>JavaScript</b></summary>

```javascript
const arr = [1, 2, 3, 4, 5];

// Method 1: Traditional for loop
for (let i = 0; i < arr.length; i++) {
  console.log(`Index ${i}: ${arr[i]}`);
}

// Method 2: for...of (value only)
for (const num of arr) {
  console.log(num);
}

// Method 3: forEach (index + value)
arr.forEach((num, i) => {
  console.log(`Index ${i}: ${num}`);
});

// Method 4: for...in (NOT recommended for arrays)
for (const i in arr) {
  console.log(arr[i]); // i is string "0", "1", etc.
}
```

</details>

### Reverse Traversal

<details open>
<summary><b>Python</b></summary>

```python
arr = [1, 2, 3, 4, 5]

# Method 1: Reverse range
for i in range(len(arr) - 1, -1, -1):
    print(arr[i])  # 5, 4, 3, 2, 1

# Method 2: Reversed iterator
for num in reversed(arr):
    print(num)

# Method 3: Negative indexing
for i in range(1, len(arr) + 1):
    print(arr[-i])  # arr[-1], arr[-2], ...
```

</details>

<details>
<summary><b>JavaScript</b></summary>

```javascript
const arr = [1, 2, 3, 4, 5];

// Method 1: Reverse for loop
for (let i = arr.length - 1; i >= 0; i--) {
  console.log(arr[i]); // 5, 4, 3, 2, 1
}

// Method 2: Reverse array (modifies original!)
arr.reverse().forEach((num) => console.log(num));

// Method 3: Create reversed copy
[...arr].reverse().forEach((num) => console.log(num));
```

</details>

### Two-Pointer Traversal

<details open>
<summary><b>Python</b></summary>

```python
arr = [1, 2, 3, 4, 5]

# Pointers moving inward
left, right = 0, len(arr) - 1

while left < right:
    print(f"Left: {arr[left]}, Right: {arr[right]}")
    left += 1
    right -= 1

# Output:
# Left: 1, Right: 5
# Left: 2, Right: 4
# (3 is middle, not processed)
```

</details>

<details>
<summary><b>JavaScript</b></summary>

```javascript
const arr = [1, 2, 3, 4, 5];

let left = 0;
let right = arr.length - 1;

while (left < right) {
  console.log(`Left: ${arr[left]}, Right: ${arr[right]}`);
  left++;
  right--;
}
```

</details>

---

## üîÑ Array Rotation

### Left Rotation

<details open>
<summary><b>Python</b></summary>

```python
def rotate_left(arr, k):
    """Rotate array left by k positions - O(n)"""
    n = len(arr)
    k = k % n  # Handle k > n

    # Method 1: Slicing (creates new array)
    return arr[k:] + arr[:k]

arr = [1, 2, 3, 4, 5]
rotated = rotate_left(arr, 2)  # [3, 4, 5, 1, 2]

# Method 2: In-place using reversal
def rotate_left_inplace(arr, k):
    """O(n) time, O(1) space"""
    n = len(arr)
    k = k % n

    def reverse(start, end):
        while start < end:
            arr[start], arr[end] = arr[end], arr[start]
            start += 1
            end -= 1

    # Reverse first k elements
    reverse(0, k - 1)
    # Reverse remaining elements
    reverse(k, n - 1)
    # Reverse entire array
    reverse(0, n - 1)

    return arr

arr = [1, 2, 3, 4, 5]
rotate_left_inplace(arr, 2)  # [3, 4, 5, 1, 2]
```

</details>

<details>
<summary><b>JavaScript</b></summary>

```javascript
function rotateLeft(arr, k) {
  const n = arr.length;
  k = k % n; // Handle k > n

  // Method 1: Slicing
  return [...arr.slice(k), ...arr.slice(0, k)];
}

let arr = [1, 2, 3, 4, 5];
let rotated = rotateLeft(arr, 2); // [3, 4, 5, 1, 2]

// Method 2: In-place using splice
function rotateLeftInPlace(arr, k) {
  const n = arr.length;
  k = k % n;

  const removed = arr.splice(0, k);
  arr.push(...removed);
  return arr;
}
```

</details>

**How Reversal Algorithm Works:**

```
Original: [1, 2, 3, 4, 5], k=2

Step 1: Reverse first k elements
[2, 1, 3, 4, 5]

Step 2: Reverse remaining elements
[2, 1, 5, 4, 3]

Step 3: Reverse entire array
[3, 4, 5, 1, 2] ‚úì
```

### Right Rotation

<details open>
<summary><b>Python</b></summary>

```python
def rotate_right(arr, k):
    """Rotate array right by k positions"""
    n = len(arr)
    k = k % n
    return arr[-k:] + arr[:-k]

arr = [1, 2, 3, 4, 5]
rotated = rotate_right(arr, 2)  # [4, 5, 1, 2, 3]

# Or use left rotation:
# Right rotation by k = Left rotation by (n-k)
```

</details>

<details>
<summary><b>JavaScript</b></summary>

```javascript
function rotateRight(arr, k) {
  const n = arr.length;
  k = k % n;
  return [...arr.slice(-k), ...arr.slice(0, -k)];
}

const arr = [1, 2, 3, 4, 5];
const rotated = rotateRight(arr, 2); // [4, 5, 1, 2, 3]
```

</details>

---

## üì¶ Jagged Arrays

**Definition:** Array of arrays with different lengths

<details open>
<summary><b>Python</b></summary>

```python
# Each sub-array can have different length
jagged = [
    [1, 2],
    [3, 4, 5, 6],
    [7],
    [8, 9, 10]
]

# Access
print(jagged[1][2])  # 5

# Can't assume all rows same length!
for row in jagged:
    print(f"Row length: {len(row)}")
    for val in row:
        print(val, end=" ")
    print()
```

</details>

<details>
<summary><b>JavaScript</b></summary>

```javascript
// Each sub-array can have different length
const jagged = [[1, 2], [3, 4, 5, 6], [7], [8, 9, 10]];

// Access
console.log(jagged[1][2]); // 5

// Can't assume all rows same length!
jagged.forEach((row) => {
  console.log(`Row length: ${row.length}`);
  row.forEach((val) => console.log(val));
});
```

</details>

---

## üíæ Cache Locality & Performance

### Why Arrays are Fast

**Sequential memory access = cache-friendly**

```
CPU Cache Structure:
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   CPU Core      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ L1 Cache ‚îÇ  (Fastest, ~0.5ns, 32-64KB)
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ L2 Cache ‚îÇ  (Fast, ~7ns, 256KB-1MB)
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ L3 Cache ‚îÇ  (Slower, ~20ns, 8-64MB)
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   RAM    ‚îÇ  (Slow, ~100ns, several GB)
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Array Access Pattern:**

```python
# Good: Sequential access (cache-friendly)
arr = [1, 2, 3, 4, 5, 6, 7, 8]
total = 0
for i in range(len(arr)):
    total += arr[i]
# CPU loads chunks: [1,2,3,4], [5,6,7,8]
# Most accesses hit cache!

# Bad: Random access (cache-unfriendly)
for i in [7, 2, 5, 1, 6, 3, 8, 4]:
    total += arr[i]
# Each access might miss cache
```

**Why it matters:**

- Cache hit: ~0.5-7 nanoseconds
- Cache miss: ~100 nanoseconds
- **20-200x performance difference!**

**Interview Gold:**

> "Arrays excel because of spatial locality. When you access arr[i], the CPU doesn't just load that element‚Äîit loads a whole cache line (typically 64 bytes, so maybe 16 integers). Sequential access hits cache; random access misses. This is why iterating an array is much faster than following pointers in a linked list."

---

## üé§ Top Interview Questions & Answers

### Q1: Why is array access O(1)?

**Perfect Answer:**

> "Array access is O(1) because of how arrays are stored in memory. Elements are stored contiguously, meaning back-to-back with no gaps. To access any element, we just need one mathematical calculation:
>
> `address = base_address + (index √ó element_size)`
>
> For example, if an integer array starts at memory address 1000, and each integer is 4 bytes, then arr[3] is at address 1000 + (3 √ó 4) = 1012. This is a single arithmetic operation, so it's constant time regardless of array size.
>
> This is also called 'random access' because any element is equally fast to reach, unlike a linked list where you must traverse from the beginning."

---

### Q2: What's the difference between static and dynamic arrays?

**Perfect Answer:**

> "Static arrays have fixed size allocated at creation and can't grow. Dynamic arrays can resize as needed.
>
> The key difference is in insertion: static arrays will overflow if you try to add more elements than allocated. Dynamic arrays automatically resize‚Äîtypically doubling capacity when full‚Äîand copy elements to the new location.
>
> This makes insertion in dynamic arrays 'amortized O(1)' rather than O(1). Occasionally you pay O(n) for a resize, but averaged over many insertions, it's constant time. If you insert 1000 elements, maybe 10 trigger resizes, so 1000 elements take maybe 2000 operations total‚Äîstill averages to O(1) per insertion.
>
> In practice, dynamic arrays (like Python lists, JavaScript arrays, Java ArrayList, C++ vector) are almost always what you want because of their flexibility."

---

### Q3: Why is inserting at the beginning of an array O(n)?

**Perfect Answer:**

> "Inserting at the beginning requires shifting all existing elements one position to the right to make space. If the array has n elements, we must move all n of them, making it O(n).
>
> Visually: to insert 0 into [1,2,3,4,5]:
>
> ```
> [1, 2, 3, 4, 5]
>  ‚Üì  ‚Üì  ‚Üì  ‚Üì  ‚Üì  (shift right)
> [_, 1, 2, 3, 4, 5]
>  ‚Üì (insert)
> [0, 1, 2, 3, 4, 5]
> ```
>
> Each element must be moved, so we do n operations. This is why if you need frequent insertions at the front, you might consider a deque or linked list instead, where adding to the front is O(1)."

---

### Q4: When would you use an array vs a linked list?

**Perfect Answer:**

> "Arrays are better when:
>
> - You need fast random access (O(1) by index)
> - You're mostly reading data, not inserting/deleting
> - You know the approximate size in advance
> - Memory efficiency matters (less overhead)
> - Cache performance matters (spatial locality)
>
> Linked lists are better when:
>
> - You need frequent insertions/deletions, especially at the beginning
> - Size changes dramatically and unpredictably
> - You never need random access
> - Memory fragmentation is okay
>
> In practice, arrays (or dynamic arrays) are the default choice for 80-90% of cases. Modern dynamic arrays handle resizing so well that their flexibility outweighs the occasional O(n) resize. I'd only reach for a linked list when I'm doing heavy insertion/deletion in the middle of the data structure, or when building specific data structures like a graph's adjacency list."

---

### Q5: How does a dynamic array achieve amortized O(1) append?

**Perfect Answer:**

> "Dynamic arrays achieve amortized O(1) by using a capacity doubling strategy. Here's how it works:
>
> When the array reaches capacity and you try to append:
>
> 1. Allocate new array with double the capacity
> 2. Copy all existing elements to new array
> 3. Add the new element
> 4. Delete the old array
>
> Let's trace the cost for 8 insertions:
>
> ```
> Insert 1: Create size 1, copy 0, insert 1 = 1 operation
> Insert 2: Resize to 2, copy 1, insert 1 = 2 operations
> Insert 3: Use space, insert 1 = 1 operation
> Insert 4: Resize to 4, copy 2, insert 1 = 3 operations
> Insert 5-7: Use space = 1 operation each = 3 operations
> Insert 8: Resize to 8, copy 4, insert 1 = 5 operations
>
> Total: 1+2+1+3+1+1+1+5 = 15 operations for 8 inserts
> Average: 15/8 ‚âà 2 operations per insert
> ```
>
> As n grows, this converges to constant time per operation. The resize cost is 'amortized' across all the insertions that don't require resizing. This is why we say amortized O(1) rather than worst-case O(1)‚Äîany single operation might be O(n), but the average is O(1)."

---

### Q6: What are the time complexities of common array operations?

**Perfect Answer:**

> "Here's my mental model:
>
> **O(1) operations:**
>
> - Access by index: arr[i]
> - Update by index: arr[i] = x
> - Append to end: arr.append(x) [amortized]
> - Remove from end: arr.pop()
>
> **O(n) operations:**
>
> - Search unsorted array
> - Insert at beginning: shifts all elements
> - Remove from beginning: shifts all elements
> - Insert/remove in middle: shifts some elements
>
> **O(log n) operations:**
>
> - Binary search on sorted array
>
> The pattern: anything involving the end is O(1), anything involving the beginning or middle potentially touches all elements so it's O(n). The exception is when the array is sorted‚Äîthen we can binary search for O(log n)."

---

## üîë Must-Know Checklist

### ‚úÖ Critical (Master These First)

- ‚úÖ Why array access is O(1)
- ‚úÖ Difference between static and dynamic arrays
- ‚úÖ Why insertion at beginning is O(n)
- ‚úÖ Amortized O(1) append in dynamic arrays
- ‚úÖ Time complexities of all basic operations
- ‚úÖ How arrays are stored in memory (contiguous)
- ‚úÖ When to use arrays vs other data structures
- ‚úÖ How to create and traverse 2D arrays

### ‚úÖ Should Know

- ‚úÖ Cache locality benefits
- ‚úÖ Array rotation algorithms
- ‚úÖ Different traversal patterns
- ‚úÖ Handling jagged arrays
- ‚úÖ In-place operations vs creating new arrays

### ‚úÖ Nice to Know (Advanced)

- [ ] Memory alignment concepts
- [ ] Sparse array representations
- [ ] Memory layout differences across languages
- [ ] When arrays outperform other structures

---

## üö® Common Mistakes to Avoid

### Mistake 1: Confusing Array Length and Capacity

```python
# Python hides this, but internally:
arr = []  # capacity: 0, length: 0
arr.append(1)  # capacity: 4, length: 1
arr.append(2)  # capacity: 4, length: 2
# ... continuing ...
arr.append(5)  # capacity: 8, length: 5

# Length = number of elements
# Capacity = space allocated
# Capacity >= Length always
```

### Mistake 2: Modifying Array While Iterating

```python
# WRONG - skips elements!
arr = [1, 2, 3, 4, 5]
for i in range(len(arr)):
    if arr[i] % 2 == 0:
        arr.pop(i)  # Changes length during iteration!

# RIGHT - iterate backwards or create new array
# Method 1: Backwards
for i in range(len(arr) - 1, -1, -1):
    if arr[i] % 2 == 0:
        arr.pop(i)

# Method 2: New array
arr = [x for x in arr if x % 2 != 0]
```

### Mistake 3: Shallow vs Deep Copy

```python
# Shallow copy - nested structures still reference same objects
arr1 = [[1, 2], [3, 4]]
arr2 = arr1.copy()  # or arr1[:]

arr2[0][0] = 999
print(arr1)  # [[999, 2], [3, 4]] - MODIFIED!

# Deep copy - fully independent
import copy
arr3 = copy.deepcopy(arr1)
arr3[0][0] = 111
print(arr1)  # [[999, 2], [3, 4]] - unchanged
```

### Mistake 4: Negative Index Confusion

```python
arr = [1, 2, 3, 4, 5]

# Python allows negative indexing
print(arr[-1])  # 5 (last element)
print(arr[-2])  # 4 (second to last)

# But JavaScript doesn't!
const jsArr = [1, 2, 3, 4, 5];
console.log(jsArr[-1]);  // undefined (NOT last element!)
```

### Mistake 5: Array Reference vs Copy

```python
# Reference - both point to same array
arr1 = [1, 2, 3]
arr2 = arr1  # NOT a copy!
arr2.append(4)
print(arr1)  # [1, 2, 3, 4] - modified!

# Copy - independent arrays
arr3 = arr1.copy()  # or arr1[:]
arr3.append(5)
print(arr1)  # [1, 2, 3, 4] - unchanged
```

---

## üí° Pro Tips for Interviews

1. **State the time complexity** of your solution even if not asked
2. **Consider edge cases**: empty array, single element, duplicates
3. **Ask about array size**: small vs large affects approach
4. **Mention space-time tradeoffs**: in-place vs new array
5. **Know built-in methods**: don't reinvent the wheel, but know complexity
6. **Draw pictures**: visualize array operations
7. **Consider sorting**: often enables better algorithms
8. **Use two pointers**: powerful pattern for many array problems
9. **Think about memory**: contiguous storage, cache effects
10. **Validate assumptions**: "Can I assume the array is sorted?"

---

## üìö Quick Reference Card

| Aspect           | Details                          |
| ---------------- | -------------------------------- |
| **Memory**       | Contiguous block                 |
| **Access**       | O(1) by index                    |
| **Search**       | O(n) unsorted, O(log n) sorted   |
| **Insert End**   | O(1) amortized                   |
| **Insert Begin** | O(n)                             |
| **Delete End**   | O(1)                             |
| **Delete Begin** | O(n)                             |
| **Space**        | O(n)                             |
| **Best For**     | Random access, cache performance |
| **Worst For**    | Frequent insertions at beginning |

---

## üéØ Key Takeaways

1. **Arrays = Random Access**: O(1) access to any element by index
2. **Contiguous = Fast**: Sequential memory access is cache-friendly
3. **End Operations = O(1)**: Append/remove at end is efficient
4. **Front/Middle = O(n)**: Requires shifting elements
5. **Dynamic Arrays**: Use capacity doubling for amortized O(1) append
6. **Sorted Arrays**: Enable O(log n) binary search
7. **Multi-dimensional**: Row-major order in memory
8. **Static vs Dynamic**: Trade flexibility for simplicity

Arrays are the foundation of almost every algorithm you'll write. Master these fundamentals and you'll recognize patterns everywhere!
